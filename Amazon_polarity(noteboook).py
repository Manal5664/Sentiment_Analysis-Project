# -*- coding: utf-8 -*-
"""Fa24-MSDS-0004 Fa24-MSDS-0021.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ymZEZKkRL78HrtTPGWnsOLqUBY8rkK1Z

Dataset Selection

Data Load
"""

from datasets import load_dataset

# Load the dataset
dataset = load_dataset("amazon_polarity")

# Take a small subset, e.g., 100,000 rows for train, 10,000 for test
train_subset = dataset['train'].shuffle(seed=42).select(range(100000))
test_subset = dataset['test'].shuffle(seed=42).select(range(10000))

# Merge title and content into a single text column
def merge_text(example):
    example['text'] = example['title'] + " " + example['content']
    return example

train_subset = train_subset.map(merge_text)
test_subset = test_subset.map(merge_text)

"""Preprocessing

Train/validation/test splits
"""

print(train_subset[0])
print(test_subset[0])
print(train_subset.column_names)

from sklearn.model_selection import train_test_split

train_texts, val_texts, train_labels, val_labels = train_test_split(
    list(train_subset['text']),
    list(train_subset['label']),
    test_size=0.1,  # 10% for validation
    random_state=42
)

test_texts = list(test_subset['text'])
test_labels = list(test_subset['label'])

print(train_texts[0])
train_labels[0]  # 1

"""Cleaning"""

import re

def clean_text(text):
    text = text.lower()
    text = re.sub(r"[^a-z0-9\s]", "", text)
    # def clean_text(example):

    # text = example["text"]

    # text = text.lower()

    text = re.sub(r"<.*?>", "", text)

    text = re.sub(r"http\S+|www\S+", "", text)

    text = re.sub(r"[^a-z\s]", "", text)

    text = re.sub(r"\s+", " ", text).strip()

    return {"clean_text":text}
    # return text

train_texts = [clean_text(text) for text in train_texts]
val_texts = [clean_text(text) for text in val_texts]
test_texts = [clean_text(text) for text in test_texts]

print(train_texts[0])

# Label Encoding is not required in this dataset.... it is already encoded
print(train_labels[0])
print(train_labels[5])

"""Normalization / tokenization"""

# tokeinzation tf-idf
from sklearn.feature_extraction.text import TfidfVectorizer

# Create the vectorizer
vectorizer = TfidfVectorizer(
    max_features=10000,  # limit vocabulary size
    ngram_range=(1, 2),  # unigrams + bigrams
)

# Extract the cleaned text from the dictionaries
train_texts_clean = [d['clean_text'] for d in train_texts]
val_texts_clean = [d['clean_text'] for d in val_texts]
test_texts_clean = [d['clean_text'] for d in test_texts]

# Fit on train and transform train + val + test
X_train = vectorizer.fit_transform(train_texts_clean)
X_val = vectorizer.transform(val_texts_clean)
X_test = vectorizer.transform(test_texts_clean)

# Labels
y_train = train_labels
y_val = val_labels
y_test = test_labels

print("Train shape:", X_train.shape)

"""Models to Implement

  (i) One ML baseline

· Logistic Regression

· Purpose: benchmarking
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Initialize the model
lr_model = LogisticRegression(
    max_iter=1000,  # iterations
    solver='saga',  # used for large datasets
    random_state=42
)

lr_model.fit(X_train, y_train)

"""Evaluation Metrics


· Accuracy, Precision, Recall, F1

"""

val_preds = lr_model.predict(X_val)

# Evaluate
val_acc = accuracy_score(y_val, val_preds)
val_prec = precision_score(y_val, val_preds)
val_rec = recall_score(y_val, val_preds)
val_f1 = f1_score(y_val, val_preds)

print("Validation Accuracy:", val_acc)
print("Validation Precision:", val_prec)
print("Validation Recall:", val_rec)
print("Validation F1 Score:", val_f1)

"""Evaluation Metrics

Depending on the task:

· Accuracy, Precision, Recall, F1

· Confusion Matrix

· ROC-AUC

· BLEU (if NLP seq2seq)

· MAE/MSE/RMSE (if regression)

MAE, MSE and RMSE  are  used for regression tasks, where the model predicts continuous numerical values. Since we are performing a classification task (predicting positive or negative sentiment), these metrics are not directly applicable. The metrics currently used for the GRU model (Accuracy, Precision, Recall, F1-Score, and Confusion Matrix) are appropriate for evaluating its performance on this classification task.
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score

test_preds = lr_model.predict(X_test)
test_probs = lr_model.predict_proba(X_test)[:, 1] # Get probabilities for the positive class

# Evaluate
test_acc = accuracy_score(y_test, test_preds)
test_prec = precision_score(y_test, test_preds)
test_rec = recall_score(y_test, test_preds)
test_f1 = f1_score(y_test, test_preds)
test_cm = confusion_matrix(y_test, test_preds)
test_roc_auc = roc_auc_score(y_test, test_probs)

print("Test Accuracy:", test_acc)
print("Test Precision:", test_prec)
print("Test Recall:", test_rec)
print("Test F1 Score:", test_f1)
print("Test ROC-AUC Score:", test_roc_auc)
print("Confusion Matrix:\n", test_cm)

"""For Deep learning
GRU (Deep Learning / Sequential Tokenization)
For GRU, we need sequence of integers for each text. We can use Keras Tokenizer:

"""

# For Deep learning
# use clean data_Set
train_texts_gru = train_texts_clean
val_texts_gru   = val_texts_clean
test_texts_gru  = test_texts_clean

# GRU (Deep Learning / Sequential Tokenization)

# For GRU, we need sequence of integers for each text. We can use Keras Tokenizer:

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Parameters
max_words = 20000   # vocabulary size
max_len = 200       # max sequence length

# Initialize tokenizer
tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
tokenizer.fit_on_texts(train_texts_gru)

# Convert text to sequences
X_train_seq = tokenizer.texts_to_sequences(train_texts_gru)
X_val_seq = tokenizer.texts_to_sequences(val_texts_gru)
X_test_seq = tokenizer.texts_to_sequences(test_texts_gru)

# Pad sequences to same length
X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')
X_val_pad = pad_sequences(X_val_seq, maxlen=max_len, padding='post', truncating='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')

# Labels
y_train = train_labels
y_val = val_labels
y_test = test_labels

print("Train sequences shape:", X_train_pad.shape)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

vocab_size = 20000    # same as tokenizer num_words
embedding_dim = 128   # size of word embeddings
max_len = 200         # same as padding length
gru_units = 64        # number of GRU units
dropout_rate = 0.3
batch_size = 64
epochs = 12

# model.summary()  # Now it will show real parameter counts

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, GRU, Dense

model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=embedding_dim),
    GRU(units=gru_units, dropout=dropout_rate, recurrent_dropout=dropout_rate),
    Dense(1, activation='sigmoid')
])

# Build with sample input
model.build(input_shape=(None, max_len))

# Compile
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Show summary
model.summary()

# don't run

import numpy as np
# train the model
early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)

history = model.fit(
    X_train_pad, np.array(y_train),
    validation_data=(X_val_pad, np.array(y_val)),
    epochs=epochs,
    batch_size=batch_size,
    callbacks=[early_stop]
)

import pickle

# GRU model save
model.save("gru_amazon_model.keras") # Changed to .keras extension
print("GRU model saved successfully!")

with open("gru_tokenizer.pkl", "wb") as f:
    pickle.dump(tokenizer, f)

from tensorflow.keras.models import load_model

model = load_model("gru_amazon_model.keras")
print("GRU model loaded")

import numpy as np

y_train = np.array(y_train)
y_val = np.array(y_val)
y_test = np.array(y_test)

test_loss, test_acc = model.evaluate(X_test_pad, y_test, batch_size=64)
print("Test Accuracy:", test_acc)

# Predict in batches to avoid memory issues
y_pred_prob = model.predict(X_test_pad, batch_size=64)
y_pred = (y_pred_prob > 0.5).astype(int)

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix

precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("Confusion Matrix:\n", cm)



# For logistic Regression
def predict_lr(sentence):
    # 1. Clean the sentence
    clean_sentence_dict = clean_text(sentence)
    clean_sentence_str = clean_sentence_dict['clean_text']

    # 2. Transform with the same TF-IDF vectorizer
    vec = vectorizer.transform([clean_sentence_str])

    # 3. Predict
    pred = lr_model.predict(vec)[0]

    # 4. Return label
    return "Positive" if pred == 1 else "Negative"

# Example usage
print(predict_lr("This product is bad!"))
print(predict_lr("very good quality."))

# for GRU
def predict_gru(sentence):
    # 1. Clean the sentence
    clean_sentence_dict = clean_text(sentence)
    clean_sentence_str = clean_sentence_dict['clean_text']

    # 2. Convert to sequence
    seq = tokenizer.texts_to_sequences([clean_sentence_str])

    # 3. Pad sequence
    pad_seq = pad_sequences(seq, maxlen=max_len, padding='post', truncating='post')

    # 4. Predict probability
    prob = model.predict(pad_seq)[0][0]

    # 5. Convert to label
    return "Positive" if prob > 0.5 else "Negative"

# Example usage
print(predict_gru("This product is amazing!"))
print(predict_gru("don't purchase this, very bad quality."))



"""One Advanced Enhancement



Transfer learning using Bert



"""

import torch
from datasets import load_dataset, Dataset
from transformers import Trainer, TrainingArguments
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

from transformers import BertTokenizerFast, BertForSequenceClassification
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# don't run because bert is already fine tuned and save
train_ds = Dataset.from_dict({
    "text": train_texts,
    "label": train_labels
})

val_ds = Dataset.from_dict({
    "text": val_texts,
    "label": val_labels
})

test_ds = Dataset.from_dict({
    "text": test_texts,
    "label": test_labels
})

# Don't run because it will upload fresh tokenzier
tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

# Don't run because it is the part of training and preprocessing
def tokenize(batch):
    # The 'text' column in the dataset contains dictionaries like {'clean_text': '...'}.
    # We need to extract the string value from these dictionaries before tokenizing.
    texts_to_tokenize = [item['clean_text'] for item in batch['text']]
    return tokenizer(
        texts_to_tokenize,
        padding="max_length",
        truncation=True,
        max_length=256
    )

# Don't run because it is the part of training and preprocessing
train_ds = train_ds.map(tokenize, batched=True)
val_ds = val_ds.map(tokenize, batched=True)
test_ds = test_ds.map(tokenize, batched=True)

# Don't because it is used to prepare the dataset for trainer
train_ds.set_format(
    type="torch",
    columns=["input_ids", "attention_mask", "label"]
)

val_ds.set_format(
    type="torch",
    columns=["input_ids", "attention_mask", "label"]
)

test_ds.set_format(
    type="torch",
    columns=["input_ids", "attention_mask", "label"]
)

# !ls

# Don't run because it load a fresh new model

model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=2
)
# model = BertForSequenceClassification.from_pretrained(
#     "./bert_amazon_model"
# )

# tokenizer = BertTokenizerFast.from_pretrained("./bert_amazon_model")
# model.eval()

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)

    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, preds, average="binary"
    )
    acc = accuracy_score(labels, preds)

    return {
        "accuracy": acc,
        "precision": precision,
        "recall": recall,
        "f1": f1
    }

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    report_to="none",
    save_total_limit=1

)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=val_ds,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()

trainer.save_model("./bert_amazon_model")
tokenizer.save_pretrained("./bert_amazon_model")

results = trainer.evaluate(test_ds)
results

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import numpy as np

# Get predictions from the trainer
predictions_output = trainer.predict(test_ds)

# Extract true labels and predicted labels
y_true = predictions_output.label_ids
y_pred_logits = predictions_output.predictions
y_pred = np.argmax(y_pred_logits, axis=1)

accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

cm = confusion_matrix(y_true, y_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()
plt.show()

from transformers import BertForSequenceClassification, BertTokenizerFast
model = BertForSequenceClassification.from_pretrained("./bert_amazon_model")
tokenizer = BertTokenizerFast.from_pretrained("./bert_amazon_model")
model.eval()
def predict_bert(sentence):
    inputs = tokenizer(
        sentence,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=256
    )

    with torch.no_grad():
        outputs = model(**inputs)

    pred_class = torch.argmax(outputs.logits, dim=1).item()
    return "Positive" if pred_class == 1 else "Negative"


print(predict_bert("I want this product!"))
print(predict_bert("Worst purchase ever, very bad quality."))

import matplotlib.pyplot as plt
import numpy as np

models = ["Logistic Regression", "GRU", "BERT"]

# Use **exact variables sent by you**
accuracy = [test_acc, test_acc, results["eval_accuracy"]]  # LR, GRU, BERT
f1_score_vals = [test_f1, f1, results["eval_f1"]]
precision_vals = [test_prec, precision, results["eval_precision"]]
recall_vals = [test_rec, recall, results["eval_recall"]]

x = np.arange(len(models))
width = 0.2

plt.figure(figsize=(10,6))
plt.bar(x - 1.5*width, accuracy, width, label='Accuracy', color='blue')
plt.bar(x - 0.5*width, f1_score_vals, width, label='F1-score', color='lightblue')
plt.bar(x + 0.5*width, precision_vals, width, label='Precision', color='gray')
plt.bar(x + 1.5*width, recall_vals, width, label='Recall', color='Purple')

plt.ylabel("Score")
plt.ylim(0, 1)
plt.title("Test Metrics Comparison: LR vs GRU vs BERT")
plt.xticks(x, models)
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()



